{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3746654-ceb1-4042-ad6a-fec7a4c07f6a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "d = pd.read_csv(\"train_dtf.csv\")  ## reads file stored in same folder as this code & makes it a dataframe.\n",
    "m = np.array(d)\n",
    "##print(m, \"this is data matrix and it's shape is\", m.shape)\n",
    "\n",
    "##plt.show()\n",
    "\n",
    "X = m[:, :2]\n",
    "##print(X, \"this is input matrix and it's shape is\", X.shape)\n",
    "X = (X - np.mean(X, axis=0)) / np.std(X, axis=0) ## standard scaling the data to improve results\n",
    "Y = m[:, 2]\n",
    "Y = np.array(Y).reshape(-1, 1)\n",
    "##Y[Y == 1] = 0\n",
    "##Y[Y == 2] = 1\n",
    "##Y[Y == 3] = 2\n",
    "##print(Y, \"this is labels matrix and it's shape is\", Y.shape)\n",
    "\n",
    "\n",
    "def one_hot(y):\n",
    "    y_int = y.astype(int)\n",
    "    num_classes = len(np.unique(y_int))\n",
    "    one_hot_encoded = np.eye(num_classes)[y_int.reshape(-1)]\n",
    "    return one_hot_encoded\n",
    "\n",
    "\n",
    "Y_o_h = one_hot(Y)\n",
    "\n",
    "##print(one_hot(Y), \"this is labels matrix one hot and it's shape is\", one_hot(Y).shape)\n",
    "\n",
    "n1 = int(input(\"enter number of neurons in first hidden layer: \"))\n",
    "n2 = int(input(\"enter number of neurons in second hidden layer: \"))\n",
    "n3 = int(input(\"enter number of neurons in third hidden layer: \"))\n",
    "n4 = int(input(\"enter number of neurons in fourth hidden layer: \"))\n",
    "n5 = int(input(\"enter number of neurons in fifth hidden layer: \"))\n",
    "alpha = float(input(\"enter floating point learning rate for gradient descent:\"))\n",
    "epochs = int(input(\"enter integer value for number of training epochs:\"))\n",
    "n = [2, n1, n2, n3, n4, n5, 3]\n",
    "np.random.seed(0)\n",
    "W1 = 0.01 * np.random.randn(n[0], n[1])\n",
    "W2 = 0.01 * np.random.randn(n[1], n[2])\n",
    "W3 = 0.01 * np.random.randn(n[2], n[3])\n",
    "W4 = 0.01 * np.random.randn(n[3], n[4])\n",
    "W5 = 0.01 * np.random.randn(n[4], n[5])\n",
    "W6 = 0.01 * np.random.randn(n[5], n[6])\n",
    "b1 = np.random.randn(1, n[1])\n",
    "b2 = np.random.randn(1, n[2])\n",
    "b3 = np.random.randn(1, n[3])\n",
    "b4 = np.random.randn(1, n[4])\n",
    "b5 = np.random.randn(1, n[5])\n",
    "b6 = np.random.randn(1, n[6])\n",
    "\n",
    "\n",
    "def softmax(inputs):\n",
    "    # Get unnormalized probabilities\n",
    "    exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "    # Normalize them for each sample\n",
    "    probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "    output = probabilities\n",
    "    return output\n",
    "\n",
    "\n",
    "def loss(pred, o_h_true):  ## categorical cross entropy loss\n",
    "    pred_clipped = np.clip(pred, 1e-7, 1 - 1e-7)\n",
    "    true_clipped = np.clip(o_h_true, 1e-7, 1 - 1e-7)\n",
    "    correct_confidences = np.sum(pred_clipped * true_clipped, axis=1)\n",
    "    negative_logs = -np.log(correct_confidences)\n",
    "    loss = np.mean(negative_logs)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def forward():\n",
    "    global Z1, A1, Z2, A2, Z3, A3, Z4, A4, Z5, A5, Z6, A6, Y_hat, W1, W2, W3, W4, W5, W6, b1, b2, b3, b4, b5, b6, Loss\n",
    "    Z1 = np.dot(X, W1) + b1\n",
    "    A1 = np.maximum(0, Z1)\n",
    "    Z2 = np.dot(A1, W2) + b2\n",
    "    A2 = np.maximum(0, Z2)\n",
    "    Z3 = np.dot(A2, W3) + b3\n",
    "    A3 = np.maximum(0, Z3)\n",
    "    Z4 = np.dot(A3, W4) + b4\n",
    "    A4 = np.maximum(0, Z4)\n",
    "    Z5 = np.dot(A4, W5) + b5\n",
    "    A5 = np.maximum(0, Z5)\n",
    "    Z6 = np.dot(A5, W6) + b6\n",
    "    Y_hat = softmax(Z6)  ## A6\n",
    "    Loss=loss(Y_hat, Y_o_h)\n",
    "    ##print(Y_hat, \"this is Y_hat, it's dimensions are:\", Y_hat.shape)\n",
    "    ##print(Loss, \"this is the categorical cross entropy loss\")\n",
    "\n",
    "def backward():\n",
    "    global dL_dW6, dL_db6, dL_dW5, dL_db5, dL_dW4, dL_db4, dL_dW3, dL_db3, dL_dW2, dL_db2, dL_dW1, dL_db1\n",
    "    samples = len(Y_hat) # counts number of rows of Y_hat\n",
    "    # Copy so we can safely modify\n",
    "    Y_1D=Y.ravel()\n",
    "    dL_dZ6 = Y_hat.copy()\n",
    "    # Calculate gradient\n",
    "    dL_dZ6[range(samples), Y_1D.astype(int)] -= 1\n",
    "    dL_dZ6 = dL_dZ6 / samples\n",
    "    dL_dW6 = np.dot(A5.T, dL_dZ6)\n",
    "    dL_db6 = np.sum(dL_dZ6, axis=0, keepdims=True)\n",
    "    dL_dA5 = np.dot(dL_dZ6, W6.T)\n",
    "    # Since we need to modify original variable,\n",
    "    # let’s make a copy of values first\n",
    "    dL_dZ5 = dL_dA5.copy()\n",
    "    # Zero gradient where input values were negative\n",
    "    dL_dZ5[Z5 <= 0] = 0\n",
    "    dL_dW5 = np.dot(A4.T, dL_dZ5)\n",
    "    dL_db5 = np.sum(dL_dZ5, axis=0, keepdims=True)\n",
    "    dL_dA4 = np.dot(dL_dZ5, W5.T)\n",
    "    # Since we need to modify original variable,\n",
    "    # let’s make a copy of values first\n",
    "    dL_dZ4 = dL_dA4.copy()\n",
    "    # Zero gradient where input values were negative\n",
    "    dL_dZ4[Z4 <= 0] = 0\n",
    "    dL_dW4 = np.dot(A3.T, dL_dZ4)\n",
    "    dL_db4 = np.sum(dL_dZ4, axis=0, keepdims=True)\n",
    "    dL_dA3 = np.dot(dL_dZ4, W4.T)\n",
    "    # Since we need to modify original variable,\n",
    "    # let’s make a copy of values first\n",
    "    dL_dZ4 = dL_dA4.copy()\n",
    "    # Zero gradient where input values were negative\n",
    "    dL_dZ4[Z4 <= 0] = 0\n",
    "    dL_dW4 = np.dot(A3.T, dL_dZ4)\n",
    "    dL_db4 = np.sum(dL_dZ4, axis=0, keepdims=True)\n",
    "    dL_dA3 = np.dot(dL_dZ4, W4.T)\n",
    "    # Since we need to modify original variable,\n",
    "    # let’s make a copy of values first\n",
    "    dL_dZ3 = dL_dA3.copy()\n",
    "    # Zero gradient where input values were negative\n",
    "    dL_dZ3[Z3 <= 0] = 0\n",
    "    dL_dW3 = np.dot(A2.T, dL_dZ3)\n",
    "    dL_db3 = np.sum(dL_dZ3, axis=0, keepdims=True)\n",
    "    dL_dA2 = np.dot(dL_dZ3, W3.T)\n",
    "    # Since we need to modify original variable,\n",
    "    # let’s make a copy of values first\n",
    "    dL_dZ2 = dL_dA2.copy()\n",
    "    # Zero gradient where input values were negative\n",
    "    dL_dZ2[Z2 <= 0] = 0\n",
    "    dL_dW2 = np.dot(A1.T, dL_dZ2)\n",
    "    dL_db2 = np.sum(dL_dZ2, axis=0, keepdims=True)\n",
    "    dL_dA1 = np.dot(dL_dZ2, W2.T)\n",
    "    # Since we need to modify original variable,\n",
    "    # let’s make a copy of values first\n",
    "    dL_dZ1 = dL_dA1.copy()\n",
    "    # Zero gradient where input values were negative\n",
    "    dL_dZ1[Z1 <= 0] = 0\n",
    "    dL_dW1 = np.dot(X.T, dL_dZ1)\n",
    "    dL_db1 = np.sum(dL_dZ1, axis=0, keepdims=True)\n",
    "\n",
    "def update_params_vanilla():\n",
    "    global W1, W2, W3, W4, W5, W6, b1, b2, b3, b4, b5, b6\n",
    "    W1 = W1 - (alpha * dL_dW1)\n",
    "    W2 = W2 - (alpha * dL_dW2)\n",
    "    W3 = W3 - (alpha * dL_dW3)\n",
    "    W4 = W4 - (alpha * dL_dW4)\n",
    "    W5 = W5 - (alpha * dL_dW5)\n",
    "    W6 = W6 - (alpha * dL_dW6)\n",
    "    b1 = b1 - (alpha * dL_db1)\n",
    "    b2 = b2 - (alpha * dL_db2)\n",
    "    b3 = b3 - (alpha * dL_db3)\n",
    "    b4 = b4 - (alpha * dL_db4)\n",
    "    b5 = b5 - (alpha * dL_db5)\n",
    "    b6 = b6 - (alpha * dL_db6)\n",
    "\n",
    "def update_params_ADAM():\n",
    "    # Hyperparameters\n",
    "    # learning rate\n",
    "    beta1 = 0.9\n",
    "    beta2 = 0.999\n",
    "    epsilon = 1e-8\n",
    "    t = 1  # time step (increment each iteration)\n",
    "\n",
    "    # --- Parameters ---\n",
    "    W = {\n",
    "        'W1': W1, 'W2': W2, 'W3': W3,\n",
    "        'W4': W4, 'W5': W5, 'W6': W6\n",
    "    }\n",
    "    b = {\n",
    "        'b1': b1, 'b2': b2, 'b3': b3,\n",
    "        'b4': b4, 'b5': b5, 'b6': b6\n",
    "    }\n",
    "\n",
    "    # --- Gradients ---\n",
    "    dW = {\n",
    "        'W1': dL_dW1, 'W2': dL_dW2, 'W3': dL_dW3,\n",
    "        'W4': dL_dW4, 'W5': dL_dW5, 'W6': dL_dW6\n",
    "    }\n",
    "    db = {\n",
    "        'b1': dL_db1, 'b2': dL_db2, 'b3': dL_db3,\n",
    "        'b4': dL_db4, 'b5': dL_db5, 'b6': dL_db6\n",
    "    }\n",
    "\n",
    "    # --- Moment estimates (initialize to zeros only once) ---\n",
    "    mW = {k: np.zeros_like(v) for k, v in W.items()}\n",
    "    vW = {k: np.zeros_like(v) for k, v in W.items()}\n",
    "    mb = {k: np.zeros_like(v) for k, v in b.items()}\n",
    "    vb = {k: np.zeros_like(v) for k, v in b.items()}\n",
    "\n",
    "    # --- Adam Update ---\n",
    "    for k in W:\n",
    "        # Update weights\n",
    "        mW[k] = beta1 * mW[k] + (1 - beta1) * dW[k]\n",
    "        vW[k] = beta2 * vW[k] + (1 - beta2) * (dW[k] ** 2)\n",
    "\n",
    "        mW_hat = mW[k] / (1 - beta1 ** t)\n",
    "        vW_hat = vW[k] / (1 - beta2 ** t)\n",
    "\n",
    "        W[k] -= alpha * mW_hat / (np.sqrt(vW_hat) + epsilon)\n",
    "\n",
    "    for k in b:\n",
    "        # Update biases\n",
    "        mb[k] = beta1 * mb[k] + (1 - beta1) * db[k]\n",
    "        vb[k] = beta2 * vb[k] + (1 - beta2) * (db[k] ** 2)\n",
    "\n",
    "        mb_hat = mb[k] / (1 - beta1 ** t)\n",
    "        vb_hat = vb[k] / (1 - beta2 ** t)\n",
    "\n",
    "        b[k] -= alpha * mb_hat / (np.sqrt(vb_hat) + epsilon)\n",
    "\n",
    "    # Increment timestep\n",
    "    t += 1\n",
    "\n",
    "def get_accuracy():\n",
    "    global accuracy\n",
    "    ##print(f\"{Y_hat} This is softmax_outputs, it's dimensions are {Y_hat.shape}\")\n",
    "    # Target (ground-truth) labels for 3 samples\n",
    "    class_targets = Y.ravel()\n",
    "    class_targets = class_targets.astype(int)\n",
    "\n",
    "    ##print(f\"{class_targets} This is class_targets, it's dimensions are {class_targets.shape}\")\n",
    "    # Calculate values along second axis (axis of index 1)\n",
    "    predictions = np.argmax(Y_hat, axis=1)\n",
    "\n",
    "    ##print(f\"{predictions} this is prediction matrix, it's dimensions: {predictions.shape}\")\n",
    "\n",
    "    accuracy = np.mean(predictions == class_targets)\n",
    "    ##print(f\"accuracy over training data is {accuracy*100}%\")\n",
    "\n",
    "epoch_count = []\n",
    "loss_count = []\n",
    "accuracy_count = []\n",
    "\n",
    "for i in range(1,epochs+1):\n",
    "    global epoch_count, loss_count    \n",
    "    forward()\n",
    "    get_accuracy()\n",
    "    accuracy_count.append(float(accuracy*100))\n",
    "    loss_count.append(float(Loss))\n",
    "    backward()\n",
    "    update_params_ADAM()\n",
    "    epoch_count.append(int(i))\n",
    "    ##print(i,\"epochs completed\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\"final loss is {Loss}\")\n",
    "\n",
    "plt.subplot(2,2,1)\n",
    "\n",
    "##print(f\"{loss_count}, this is all losses\")\n",
    "##print(f\"{epoch_count}, this is all epochs\")\n",
    "# Create a 2x2 grid layout\n",
    "##plt.figure(figsize=(8, 4))\n",
    "\n",
    "# First plot spans the top row (2 columns)\n",
    "##ax1 = plt.subplot2grid((2, 2), (0, 0), colspan=2)\n",
    "plt.plot(epoch_count,loss_count)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Losses against Epochs\")\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "plt.plot(epoch_count,accuracy_count)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy (in %age)\")\n",
    "plt.title(\"Losses against Accuracy\")\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "plt.scatter(m[:, 0], m[:, 1], c=m[:, 2], cmap='brg')\n",
    "plt.title(\"Data for Training\")\n",
    "\n",
    "plt.subplot(2,2,4)\n",
    "plt.scatter(x=m[:, 0],y=m[:, 1], c=predictions, cmap='brg')\n",
    "plt.title(\"How the ANN classifies it\")\n",
    "\n",
    "plt.tight_layout() ##prevents overwriting in graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9097d518-95ab-4fda-9313-fc96b04c1707",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
